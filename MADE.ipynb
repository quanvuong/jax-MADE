{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An implementation of Masked Autoencoder for Density Estimation\n",
    "# inspired by https://github.com/karpathy/pytorch-made\n",
    "\n",
    "import jax.numpy as np\n",
    "import jax.random as random\n",
    "import jax.nn as nn\n",
    "from jax import device_put, grad, jit, random, vmap\n",
    "\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import numpy as onp\n",
    "import numpy.random as onpr\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = onp.load('./binarized_mnist.npz')\n",
    "xtr, xte = mnist['train_data'], mnist['valid_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(nin,\n",
    "             hidden_sizes, seed, natural_ordering):\n",
    "    # Return a randomly generated mask\n",
    "    # the mask generated is a deterministic function of the seed\n",
    "    \n",
    "    rng = onpr.RandomState(seed)\n",
    "    L = len(hidden_sizes)\n",
    "    \n",
    "    m = {}\n",
    "    \n",
    "    # Sample the order of the input\n",
    "    m[-1] = onp.arange(nin) if natural_ordering else rng.permutation(nin)\n",
    "    \n",
    "    # Sample the connectivity of all hidden layers\n",
    "    for l in range(L):\n",
    "        \n",
    "        # For each unit in layer l,\n",
    "        # it can be connected to at most nin \n",
    "        # and at least m[l-1] dimension in the input\n",
    "        m[l] = rng.randint(\n",
    "            m[l-1].min(), nin-1, \n",
    "            size=hidden_sizes[l]\n",
    "        )\n",
    "        \n",
    "    # Construct the mask matrices\n",
    "    masks = []\n",
    "    for l in range(L):\n",
    "        \n",
    "        # The mask at each hidden layer is 1_{m^l >= m^{l-1}}\n",
    "        masks.append(\n",
    "            m[l-1][:, None] <= m[l][None, :]\n",
    "        )\n",
    "        \n",
    "    # Construct the mask at the output layer\n",
    "    # 1_{d > m^L}\n",
    "    masks.append(\n",
    "        m[L-1][:, None] < m[-1][None, :]\n",
    "    )\n",
    "        \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters and optim\n",
    "D = xtr.shape[1]\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    hid_size = 4\n",
    "    hidden_sizes = [] \n",
    "    sizes = [D, D]\n",
    "\n",
    "else:\n",
    "    hid_size = 500\n",
    "    hidden_sizes = [hid_size, hid_size] \n",
    "    sizes = [D, hid_size, hid_size, D]\n",
    "\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "params = init_network_params(sizes, key)\n",
    "\n",
    "init_fnc, update_fnc, get_params = optimizers.adam(step_size=0.001)\n",
    "\n",
    "opt_state = init_fnc(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define forward pass of the model\n",
    "def forward(ipt, ps, ms):\n",
    "    # ps, ms are param and mask respectively\n",
    "    \n",
    "    activation = ipt \n",
    "    \n",
    "    for (w, b), m in zip(ps[:-1], ms[:-1]):\n",
    "        \n",
    "        masked_weight = np.multiply(w, m.T)\n",
    "        out = np.dot(masked_weight, activation) + b\n",
    "        activation = nn.relu(out)\n",
    "        \n",
    "    final_w, final_b = ps[-1]\n",
    "    final_m = ms[-1]\n",
    "    \n",
    "    masked_w = np.multiply(final_w, final_m.T)\n",
    "    logits = np.dot(masked_w, activation) + final_b\n",
    "    return logits \n",
    "\n",
    "b_forward = vmap(forward, in_axes=(0, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def binary_cross_entropy_with_logits(target, logit):\n",
    "    \n",
    "    eps = 1e-7\n",
    "    \n",
    "    # y log sigmoid(x) + (1 - y) log (1 - sigmoid(x))\n",
    "    sig = nn.sigmoid(logit)\n",
    "    \n",
    "    # clipping for numerical stability\n",
    "    sig = np.clip(sig, eps, 1.0 - eps)\n",
    "    \n",
    "    return - (target * np.log(sig) + (1.0 - target) * np.log(1.0 - sig))\n",
    "\n",
    "\n",
    "b_binary_cross_entropy_with_logits = vmap(binary_cross_entropy_with_logits, \n",
    "                                          in_axes=(0, 0))\n",
    "\n",
    "def loss_fnc(params, masks, b_x):\n",
    "    \n",
    "    b_logits = b_forward(b_x, params, masks)\n",
    "    loss = b_binary_cross_entropy_with_logits(b_x.flatten(), b_logits.flatten())\n",
    "    loss = np.sum(loss) / len(b_x)\n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "train loss:  219.04561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 1/51 [00:01<01:09,  1.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  170.365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 2/51 [00:01<00:56,  1.15s/it]\u001b[A\n",
      "  6%|▌         | 3/51 [00:02<00:48,  1.00s/it]\u001b[A\n",
      "  8%|▊         | 4/51 [00:03<00:42,  1.11it/s]\u001b[A\n",
      " 10%|▉         | 5/51 [00:03<00:38,  1.20it/s]\u001b[A\n",
      " 12%|█▏        | 6/51 [00:04<00:34,  1.29it/s]\u001b[A\n",
      " 14%|█▎        | 7/51 [00:05<00:31,  1.40it/s]\u001b[A\n",
      " 16%|█▌        | 8/51 [00:05<00:29,  1.45it/s]\u001b[A\n",
      " 18%|█▊        | 9/51 [00:06<00:29,  1.44it/s]\u001b[A\n",
      " 20%|█▉        | 10/51 [00:07<00:27,  1.48it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-514f73764806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnsteps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define the training loop\n",
    "\n",
    "N = xtr.shape[0]\n",
    "B = 100\n",
    "nsteps = N//B \n",
    "\n",
    "xte_subset = xte[:5]\n",
    "\n",
    "\n",
    "from jax.experimental.optimizers import l2_norm\n",
    "\n",
    "\n",
    "\n",
    "def loss_fnc_with_reg(params, masks, b_x):\n",
    "    \n",
    "    loss = loss_fnc(params, masks, b_x)\n",
    "    \n",
    "    return loss + 1e-4 * l2_norm(params)\n",
    "\n",
    "\n",
    "@jit\n",
    "def update(update_idx, opt_state, masks, b_x):\n",
    "    \n",
    "    params = get_params(opt_state)\n",
    "\n",
    "    grads = grad(loss_fnc_with_reg)(params, masks, b_x)   \n",
    "\n",
    "    return update_fnc(update_idx, grads, opt_state)\n",
    "\n",
    "\n",
    "@jit\n",
    "def j_loss_fnc(opt_state, masks, b_x):\n",
    "    \n",
    "    params = get_params(opt_state)\n",
    "    loss = loss_fnc(params, masks, b_x)\n",
    "    return loss\n",
    "\n",
    "\n",
    "masks = get_mask(D, hidden_sizes, 1, True)\n",
    "\n",
    "\n",
    "for epoch in trange(51):\n",
    "    \n",
    "    losses = []\n",
    "    for step in range(nsteps):\n",
    "\n",
    "        b_x = xtr[step*B:step*B+B]\n",
    "        \n",
    "        loss = j_loss_fnc(opt_state, masks, b_x)\n",
    "        losses.append(loss)\n",
    "\n",
    "        opt_state = update(epoch * nsteps + step, opt_state, masks, b_x)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch', epoch)\n",
    "        print('train loss: ', np.mean(np.array(losses)))\n",
    "        print('test loss: ', j_loss_fnc(opt_state, masks, xte))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
