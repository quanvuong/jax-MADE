{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An implementation of Masked Autoencoder for Density Estimation\n",
    "# inspired by https://github.com/karpathy/pytorch-made\n",
    "\n",
    "import jax.numpy as np\n",
    "import jax.random as random\n",
    "import jax.nn as nn\n",
    "from jax import device_put, grad, jit, random, vmap\n",
    "\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import numpy as onp\n",
    "import numpy.random as onpr\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = onp.load('./binarized_mnist.npz')\n",
    "xtr, xte = mnist['train_data'], mnist['valid_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(nin,\n",
    "             hidden_sizes, seed, natural_ordering):\n",
    "    # Return a randomly generated mask\n",
    "    # the mask generated is a deterministic function of the seed\n",
    "    \n",
    "    rng = onpr.RandomState(seed)\n",
    "    L = len(hidden_sizes)\n",
    "    \n",
    "    m = {}\n",
    "    \n",
    "    # Sample the order of the input\n",
    "    m[-1] = onp.arange(nin) if natural_ordering else rng.permutation(nin)\n",
    "    \n",
    "    # Sample the connectivity of all hidden layers\n",
    "    for l in range(L):\n",
    "        \n",
    "        # For each unit in layer l,\n",
    "        # it can be connected to at most nin \n",
    "        # and at least m[l-1] dimension in the input\n",
    "        m[l] = rng.randint(\n",
    "            m[l-1].min(), nin-1, \n",
    "            size=hidden_sizes[l]\n",
    "        )\n",
    "        \n",
    "    # Construct the mask matrices\n",
    "    masks = []\n",
    "    for l in range(L):\n",
    "        \n",
    "        # The mask at each hidden layer is 1_{m^l >= m^{l-1}}\n",
    "        masks.append(\n",
    "            m[l-1][:, None] <= m[l][None, :]\n",
    "        )\n",
    "        \n",
    "    # Construct the mask at the output layer\n",
    "    # 1_{d > m^L}\n",
    "    masks.append(\n",
    "        m[L-1][:, None] < m[-1][None, :]\n",
    "    )\n",
    "        \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters and optim\n",
    "D = xtr.shape[1]\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    hid_size = 4\n",
    "    hidden_sizes = [] \n",
    "    sizes = [D, D]\n",
    "\n",
    "else:\n",
    "    hid_size = 500\n",
    "    hidden_sizes = [hid_size] \n",
    "    sizes = [D, hid_size, D]\n",
    "\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "params = init_network_params(sizes, key)\n",
    "\n",
    "init_fnc, update_fnc, get_params = optimizers.adam(step_size=0.001)\n",
    "\n",
    "opt_state = init_fnc(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define forward pass of the model\n",
    "def forward(ipt, ps, ms):\n",
    "    # ps, ms are param and mask respectively\n",
    "    \n",
    "    activation = ipt \n",
    "    \n",
    "    for (w, b), m in zip(ps[:-1], ms[:-1]):\n",
    "        \n",
    "        masked_weight = np.multiply(w, m.T)\n",
    "        out = np.dot(masked_weight, activation) + b\n",
    "        activation = nn.relu(out)\n",
    "        \n",
    "    final_w, final_b = ps[-1]\n",
    "    final_m = ms[-1]\n",
    "    \n",
    "    masked_w = np.multiply(final_w, final_m.T)\n",
    "    logits = np.dot(masked_w, activation) + final_b\n",
    "    return logits \n",
    "\n",
    "b_forward = vmap(forward, in_axes=(0, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def binary_cross_entropy_with_logits(target, logit):\n",
    "    \n",
    "    eps = 1e-7\n",
    "    \n",
    "    # y log sigmoid(x) + (1 - y) log (1 - sigmoid(x))\n",
    "    sig = nn.sigmoid(logit)\n",
    "    \n",
    "    # clipping for numerical stability\n",
    "    sig = np.clip(sig, eps, 1.0 - eps)\n",
    "    \n",
    "    return - (target * np.log(sig) + (1.0 - target) * np.log(1.0 - sig))\n",
    "\n",
    "\n",
    "b_binary_cross_entropy_with_logits = vmap(binary_cross_entropy_with_logits, \n",
    "                                          in_axes=(0, 0))\n",
    "\n",
    "def loss_fnc(params, masks, b_x):\n",
    "    \n",
    "    b_logits = b_forward(b_x, params, masks)\n",
    "    loss = b_binary_cross_entropy_with_logits(b_x.flatten(), b_logits.flatten())\n",
    "    loss = np.sum(loss) / len(b_x)\n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "train loss:  216.67427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/51 [00:01<01:17,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  152.27048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/51 [00:06<00:23,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10\n",
      "train loss:  102.56361\n",
      "test loss:  104.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 21/51 [00:12<00:16,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20\n",
      "train loss:  99.15046\n",
      "test loss:  102.308304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 31/51 [00:18<00:11,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30\n",
      "train loss:  97.79139\n",
      "test loss:  101.80868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 41/51 [00:23<00:05,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40\n",
      "train loss:  97.048065\n",
      "test loss:  101.68934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:28<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50\n",
      "train loss:  96.56069\n",
      "test loss:  101.697495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# define the training loop\n",
    "\n",
    "N = xtr.shape[0]\n",
    "B = 100\n",
    "nsteps = N//B \n",
    "\n",
    "xte_subset = xte[:5]\n",
    "\n",
    "\n",
    "from jax.experimental.optimizers import l2_norm\n",
    "\n",
    "\n",
    "\n",
    "def loss_fnc_with_reg(params, masks, b_x):\n",
    "    \n",
    "    loss = loss_fnc(params, masks, b_x)\n",
    "    \n",
    "    return loss + 1e-4 * l2_norm(params)\n",
    "\n",
    "\n",
    "@jit\n",
    "def update(update_idx, opt_state, masks, b_x):\n",
    "    \n",
    "    params = get_params(opt_state)\n",
    "\n",
    "    grads = grad(loss_fnc_with_reg)(params, masks, b_x)   \n",
    "\n",
    "    return update_fnc(update_idx, grads, opt_state)\n",
    "\n",
    "\n",
    "@jit\n",
    "def j_loss_fnc(opt_state, masks, b_x):\n",
    "    \n",
    "    params = get_params(opt_state)\n",
    "    loss = loss_fnc(params, masks, b_x)\n",
    "    return loss\n",
    "\n",
    "\n",
    "masks = get_mask(D, hidden_sizes, 1, True)\n",
    "\n",
    "\n",
    "for epoch in trange(51):\n",
    "    \n",
    "    losses = []\n",
    "    for step in range(nsteps):\n",
    "\n",
    "        b_x = xtr[step*B:step*B+B]\n",
    "        \n",
    "        loss = j_loss_fnc(opt_state, masks, b_x)\n",
    "        losses.append(loss)\n",
    "\n",
    "        opt_state = update(epoch * nsteps + step, opt_state, masks, b_x)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch', epoch)\n",
    "        print('train loss: ', np.mean(np.array(losses)))\n",
    "        print('test loss: ', j_loss_fnc(opt_state, masks, xte))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
